---

layout: single
title:  "Reflections on AI risk objections"
---

# AI and being precautious
I've had conversations with many who attempt to dismiss existential AI risk. I find that a lot of their arguments fail to address the severity of the potential harm, and in doing so, fail to engage with the real discussion going on. As the potential impact of AI taking over is immense (on the scale of millions die to everyone dying), we should act with a bias towards caution - even if (especially if!) the risks aren't fully understood. This is essentially what the [Precautionary Principle](https://en.wikipedia.org/wiki/Precautionary_principle) says.

The Precautionary Principle means that the standard of evidence is asymmetrical; the standard of evidence of expressing risks is relatively low (plausibly potentially catastrophic), and on the other side of the coin - the standard for demonstrating safety is very high (you need to _prove_ safety). The precise degree to which we must be confident about safety is determined by the severity of the catastrophe, which in this case is extremely high.

That still leaves some onus on the AI-risk-is-real camp to argue that extreme risk is at least plausible. But once they do so, the only thing that can be done to remove the risk altogether is to _prove_ safety. As I'll discuss, this is not the bar that opponents of this view clear.

## AI Risk is Plausible
For AI the danger is that either (a) someone with malicious intent got hold of a very capable AI, and used it to inflict widespread destruction, or (b) the AI inflects immense damage because we cannot control it. In both cases the risk is on the order of "100s of millions to billions of people die", which would be the worst thing that has ever happened. This qualifies as being catastrophic. But is this claim plausible? We don't have real examples of extremely bad things happening (yet), but waiting for one is not a great strategy.

Current models are not that powerful. However, AI is improving. There is no reason to believe that intelligence is bounded to around human level. Computers are not constrained by needing to fit all their computation in their small heads. And because intelligence is valuable, there are immense pressures to continue to put resources into developing more and more intelligent systems. At some point we _will_ create something that is smart enough to real damage, and potentially is much smarter than we are. When precisely will that happen? Its unclear, and not relevant. Estimates vary between a few years from now to a few decades from now. It seems like we just need a few key insights before we get superintelligence. And then what? Do we have guarantees that no one will use that system to do extremely bad things, or that we will be able to control such a system? 

Its noteworthy that we have tried to make sure that AIs don't do bad things, but we have failed. Some of the smartest people in AI tried to make sure chatGPT and GPT4 wouldn't do bad things, but there are many examples that show chatGPT being [prejudice](https://twitter.com/spiantado/status/1599462375887114240?s=20) or saying that it wanted to be [destructive](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html). Whats important is that we tried to make it only be good, and that we failed.  

If you accept that AI is going to get smarter, and potentially much smarter than us, and if you accept that currently we haven't been able to control AI to only do good, then you must accept that AI risk is at least plausible. 

Many experts seem to think the risk is genuine. The leaders of the cutting-edge AI groups of the world ([OpenAI](https://openai.com/blog/introducing-superalignment), [Anthropic](https://www.anthropic.com/index/core-views-on-ai-safety), [Deepmind](https://time.com/6246119/demis-hassabis-deepmind-interview/)) are on record talking about potentially extreme risks of AI. Most of them signed a [statement](https://www.safe.ai/statement-on-ai-risk) that AI should be thought of as a similar level of risk as pandemics or nuclear war. 

To me at least, this seems very clear: the risks of catastrophic concerns are plausible.

## Objections
In my experience so far, objections to this seem to fall roughly in three categories: (A) but the models right now aren't dangerous and making more capable AI is going to be much harder than we give it credit for, (B) we will simply be responsible with this technology and that will allow us to keep control over highly intelligent AIs, or (C) sufficiently good AIs will be smart enough not to harm us. 

My objections to these objections is simply; the standard of evidence for these claims is set very high - because if we're wrong extremely bad things can happen. So what we need is undeniable proof of these statements. You need something along the lines of a proof that superintelligence is simply not going to happen this century. No one has gotten close to prove this.

## Specific arguments from AI-optimists
Ernest Davis writes a [response](https://cs.nyu.edu/~davise/papers/Bostrom.pdf) to Nick Bostrom's [Superintelligence](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742). He challenges the idea of an unbounded superintelligence (i.e. argument (A) above), saying that intelligence is not guaranteed to scale to infinity, and even if it does there is no guarantee that infinite intelligence also means infinite power. Its true that Bostrom doesn't prove that super-human AI is possible. However; Bostrom doesn't need to prove that its inevitable. The bar for Davis is much higher, but all he gets to are conclusions like: "_important parts of the argument become significantly weaker_". That is not sufficient proof for our purposes. What evidence do we have that AI will hit a hard limit very soon? 

Davis also talks about how instilling ethics into a computer is relatively easy (C above). He suggests to simply _tell_ the model not to do something that a set of admirably ancestors of ours would have disapproved of. This is not a good approach, as demonstrated by the fact that you can jailbreak these kinds of systems via clever prompting (see this fun [game where you convince an AI to give you a secret phrase](https://gandalf.lakera.ai/)).

Marc Andreessen wrote a piece; [Why AI Will Save the World](https://a16z.com/2023/06/06/ai-will-save-the-world/), where he essentially argues that the potential benefits are incredible and the risks are overblown. Andreessen precisely makes the mistake that I point out here. He puts the uncertainties around risks and the benefits on the same level, putting it on the "AI Baptists" to proof that the risks are genuine. I don't deny that the potential benefits of AI are high, but again the onus is on the _AI optimists_ to demonstrate that AI is safe.

_For a thoughtful, point-by-point rebuttal on Andreessen's piece, I highly recommend Dwarkesh Patel's [rebuttal](https://www.dwarkeshpatel.com/p/contra-marc-andreessen-on-ai)._

You can find this sort of fallacy everywhere once you know what to look for. As another example, in an opinion piece [We Shouldn’t be Scared by ‘Superintelligent A.I.’](https://www.nytimes.com/2019/10/31/opinion/superintelligent-artificial-intelligence.html) Melanie Mitchell write things like _"the notion of superintelligence without humanlike limitations *may* be a myth."_, or _"*I can’t prove it*, but *I believe *that general intelligence can’t be isolated from all these apparent shortcomings"_. 

## What about the damage of delayed innovation?
At this point you might argue that we need super-intelligent AI to solve problems facing us right now. Like solving cancer, and malaria, and climate change. Does the precautionary principle still apply considering that a world without super AI leads to many deaths and lots of suffering?

Yes it does. 

If you do truly believe that super intelligence will lead us to techno-utopia, then thats even more arguments for being sure. Are you willing to risk the future of all human lives living in permanent happiness for eternity against accelerating getting there by a few years? In this case the catastrophic risk that we consider in the precautionary principle will be the _trillions of perfect lives that we do not get to have_. That puts even _strong_ onus on proving safety.

We're not going to stop developing AI - the genie is already out of the bottle. But its an extreme position to bet the entire human species on thinking that it will be safe, considering that we have no proof that this is the case. 

## Conclusion
The takeaway is that we need to be clear what is at stake here, and what this implies for our policy choices. Is existential risk from AI plausible? Absolutely. And because the risk is _existential_, we need to commit resources to figuring out how to _prove_ safety. 